{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens\n",
    "sent_tokens = text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \" Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book\",\n",
       " ' It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged',\n",
       " ' It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens_2 = sent_tokenize(text) # What is stored in this ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokens_2)   # why length 4 ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \".Hello how are you. Diff.between me and you. Is different. Hope you doing well. Will  try to meet soon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens_2 = text_2.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Hello how are you',\n",
       " ' Diff',\n",
       " 'between me and you',\n",
       " ' Is different',\n",
       " ' Hope you doing well',\n",
       " ' Will  try to meet soon',\n",
       " '']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_2token_3 = sent_tokenize(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.Hello how are you.',\n",
       " 'Diff.between me and you.',\n",
       " 'Is different.',\n",
       " 'Hope you doing well.',\n",
       " 'Will  try to meet soon.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_2token_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_2token_3[0].startswith(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        words.append(word.lower())\n",
    "    return words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    final_list = []\n",
    "    for word in words:\n",
    "        if word not in stopwords_list:\n",
    "            final_list.append(word)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenize(text)\n",
    "final_list = remove_stopwords(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(text):\n",
    "    tf = dict()\n",
    "    for word in final_list:\n",
    "        if word in tf:\n",
    "            tf[word] += 1\n",
    "        else:\n",
    "            tf[word] = 1\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lorem': 4, 'ipsum': 3, 'simply': 1, 'dummy': 2, 'text': 2, 'printing': 1, 'typesetting': 1, 'industry.': 1, \"industry's\": 1, 'standard': 1, 'ever': 1, 'since': 1, '1500s,': 1, 'unknown': 1, 'printer': 1, 'took': 1, 'galley': 1, 'type': 2, 'scrambled': 1, 'make': 1, 'specimen': 1, 'book.': 1, 'survived': 1, 'five': 1, 'centuries,': 1, 'also': 1, 'leap': 1, 'electronic': 1, 'typesetting,': 1, 'remaining': 1, 'essentially': 1, 'unchanged.': 1, 'popularised': 1, '1960s': 1, 'release': 1, 'letraset': 1, 'sheets': 1, 'containing': 1, 'passages,': 1, 'recently': 1, 'desktop': 1, 'publishing': 1, 'software': 1, 'like': 1, 'aldus': 1, 'pagemaker': 1, 'including': 1, 'versions': 1, 'ipsum.': 1}\n"
     ]
    }
   ],
   "source": [
    "print(tf(text))  # Why output is different ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(term, doc):\n",
    "#    print(\"Divide {} / {}\".format(final_list.count(term.lower()),len(final_list)))\n",
    "    return final_list.count(term.lower()) / len(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07017543859649122"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text('lorem', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize for lorem is 0.070\n",
      "Normalize for ipsum is 0.053\n",
      "Normalize for simply is 0.018\n",
      "Normalize for dummy is 0.035\n",
      "Normalize for text is 0.035\n",
      "Normalize for printing is 0.018\n",
      "Normalize for typesetting is 0.018\n",
      "Normalize for industry. is 0.018\n",
      "Normalize for lorem is 0.070\n",
      "Normalize for ipsum is 0.053\n",
      "Normalize for industry's is 0.018\n",
      "Normalize for standard is 0.018\n",
      "Normalize for dummy is 0.035\n",
      "Normalize for text is 0.035\n",
      "Normalize for ever is 0.018\n",
      "Normalize for since is 0.018\n",
      "Normalize for 1500s, is 0.018\n",
      "Normalize for unknown is 0.018\n",
      "Normalize for printer is 0.018\n",
      "Normalize for took is 0.018\n",
      "Normalize for galley is 0.018\n",
      "Normalize for type is 0.035\n",
      "Normalize for scrambled is 0.018\n",
      "Normalize for make is 0.018\n",
      "Normalize for type is 0.035\n",
      "Normalize for specimen is 0.018\n",
      "Normalize for book. is 0.018\n",
      "Normalize for survived is 0.018\n",
      "Normalize for five is 0.018\n",
      "Normalize for centuries, is 0.018\n",
      "Normalize for also is 0.018\n",
      "Normalize for leap is 0.018\n",
      "Normalize for electronic is 0.018\n",
      "Normalize for typesetting, is 0.018\n",
      "Normalize for remaining is 0.018\n",
      "Normalize for essentially is 0.018\n",
      "Normalize for unchanged. is 0.018\n",
      "Normalize for popularised is 0.018\n",
      "Normalize for 1960s is 0.018\n",
      "Normalize for release is 0.018\n",
      "Normalize for letraset is 0.018\n",
      "Normalize for sheets is 0.018\n",
      "Normalize for containing is 0.018\n",
      "Normalize for lorem is 0.070\n",
      "Normalize for ipsum is 0.053\n",
      "Normalize for passages, is 0.018\n",
      "Normalize for recently is 0.018\n",
      "Normalize for desktop is 0.018\n",
      "Normalize for publishing is 0.018\n",
      "Normalize for software is 0.018\n",
      "Normalize for like is 0.018\n",
      "Normalize for aldus is 0.018\n",
      "Normalize for pagemaker is 0.018\n",
      "Normalize for including is 0.018\n",
      "Normalize for versions is 0.018\n",
      "Normalize for lorem is 0.070\n",
      "Normalize for ipsum. is 0.018\n"
     ]
    }
   ],
   "source": [
    "nm_list = []\n",
    "for term in final_list:\n",
    "    normal = normalize_text(term,text)\n",
    "    print(\"Normalize for %s is %.3f\"%(term,normal))\n",
    "    nm_list.append(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
